{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jbstCZ65bb6W",
        "q2h7rTLMx2s7",
        "J9hE7Pg5i6Yv",
        "vtummxEo_oNV",
        "DH_VRS5mypfN",
        "armW7nk8oNR9",
        "FSU67DcDoNkk",
        "lpEoFNHAwqmE",
        "jSyC074kwq1U",
        "gi1V1JVzwq6J",
        "5BRUegxPwrGk"
      ],
      "authorship_tag": "ABX9TyPS/e6Arel3vh9uNCnzatX7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ken-huea/Language-processing/blob/main/%E2%91%A1%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86%E3%81%A8%E3%83%99%E3%82%AF%E3%83%88%E3%83%AB%E8%A1%A8%E7%8F%BE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 文書のベクトル表現\n",
        "文章の類似度を確認。複数の文章を数式化してどれとどれが近しいか。と。\n",
        "\n",
        "単語の頻出や重みなどの手法で重要度は分かるだろうけど、それがどうベクトルに働くのか？"
      ],
      "metadata": {
        "id": "pXUL3yyHxz3m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### イントロ"
      ],
      "metadata": {
        "id": "jbstCZ65bb6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "①形態素解析　⇒　②文章を数字に変換(ベクトル表現)　⇒　③numpyで比較"
      ],
      "metadata": {
        "id": "Foha60era4rw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ベクトル表現に変換する方法\n",
        "\n",
        "・**カウント表現**：文書中の<font color=\"red\">**各単語の出現数**</font>に着目する方法\n",
        "\n",
        "・**バイナリ表現**：出現頻度を気にせず、文章中に各単語が<font color=\"red\">**出現したかどうか**</font>のみに着目する方法\n",
        "\n",
        "・**tf-idf表現**：tf-idfという手法で計算された、文章中の<font color=\"red\">**各単語の重み情報を扱う**</font>方法"
      ],
      "metadata": {
        "id": "AV1eRa3m0ZLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**tf**</font>：重みを付ける事。\n",
        "\n",
        "文章に｢犬｣が頻出したら、｢この犬という単語は重要なんだな｣と思うじゃん。\n",
        "\n",
        "このように単語に重みを持たせる事。\n",
        "\n",
        "だけど、助詞の「は」「が」「と」等はどこにでも頻出するので、文章の特徴とは言い難い。このような場合は重みを軽くする。みたいな。"
      ],
      "metadata": {
        "id": "-5onyCTY3kIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ベクトル表現①　Bag of Words(BOW)カウント表現\n",
        "各単語の出現回数をカウントしベクトル化⇒<font color=\"red\">不明質問</font>"
      ],
      "metadata": {
        "id": "q2h7rTLMx2s7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "例\n",
        "- John likes to watch movies. Mary likes movies too.\n",
        "- John also likes to watch football games.\n",
        "\n",
        "`[\"John\",\"likes\",\"to\",\"watch\",\"movies\",\"Mary\",\"too\",\"also\",\"football\", \"games\"]`  ⇒羅列して\n",
        "\n",
        "1. [1, 2, 1, 1, 2, 1, 1, 0, 0, 0]　　⇒単語をカウントしてる likesとwatchが２回。⇒だから重要みたい。\n",
        "2. [1, 1, 1, 1, 0, 0, 0, 1, 1, 1]\n",
        "\n",
        "\n",
        "単語の出現回数は紛れもなく文章の特徴です。"
      ],
      "metadata": {
        "id": "b-cxuQoZfQp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "欠点\n",
        "\n",
        "・単語の語順情報がない\n",
        "　・単語の意味の表現が苦手\n",
        "\n",
        "この欠点を**Doc2Vec**は補っている。"
      ],
      "metadata": {
        "id": "-_2WOebu00bg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install janome\n",
        "# ライブラリ読み込み　今回はジャノメ\n",
        "from janome.tokenizer import Tokenizer\n",
        "from gensim import corpora"
      ],
      "metadata": {
        "id": "mUycJgZi7C2C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a19faf6-3084-4056-eb8a-c2214fc869d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting janome\n",
            "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7 MB 6.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: janome\n",
            "Successfully installed janome-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 実装\n",
        "[お手本コード](https://premium.aidemy.jp/courses/5050/exercises/rkUQc38sIxf)"
      ],
      "metadata": {
        "id": "J9hE7Pg5i6Yv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "例題：\n",
        "\n",
        "・すもももももももものうち\n",
        "\n",
        "・料理も景色もすばらしい\n",
        "\n",
        "・私の趣味は写真撮影です\n",
        "\n",
        "上記の3つの文を①janomeを使い形態素解析する。 　　②カウント表現 のBag of Wordsを作成し表示する。"
      ],
      "metadata": {
        "id": "nbOwvaCo3l6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"すもももももももものうち\"\n",
        "text2 = \"料理も景色もすばらしい\"\n",
        "text3 = \"私の趣味は写真撮影です\""
      ],
      "metadata": {
        "id": "9HGMf5aedIbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = Tokenizer()\n",
        "# 分かち書き ⇒ wakati=True\n",
        "tokens1 = t.tokenize(text1, wakati=True)\n",
        "tokens2 = t.tokenize(text2, wakati=True)\n",
        "tokens3 = t.tokenize(text3, wakati=True)\n",
        "\n",
        "# tokens1 = t.tokenize(text1)\n",
        "# tokens2 = t.tokenize(text2)\n",
        "# tokens3 = t.tokenize(text3)"
      ],
      "metadata": {
        "id": "CDvae3fndTqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ジェネレーターなのでfor文で表示させる事。[詳細はこちら](https://qiita.com/keitakurita/items/5a31b902db6adfa45a70)"
      ],
      "metadata": {
        "id": "HkotAIyjidJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(tokens1))\n",
        "for token in tokens1:\n",
        "    print(token)\n",
        "\n",
        "print()\n",
        "\n",
        "print(type(tokens2))\n",
        "for token in tokens2:\n",
        "    print(token)\n",
        "\n",
        "print()\n",
        "\n",
        "print(type(tokens3))\n",
        "for token in tokens3:\n",
        "    print(token)"
      ],
      "metadata": {
        "id": "I8NvCm2wda1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 上のコードを実装させるとリストが空になる…。\n",
        "# 文章をまとめてリスト化し\n",
        "documents = [tokens1, tokens2, tokens3]\n",
        "\n",
        "# corporaを使い単語辞書を作成\n",
        "dictionary = corpora.Dictionary(documents)\n",
        "\n",
        "# 各単語のidを表示してください\n",
        "print(dictionary.token2id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-EuQM_Th8Qv",
        "outputId": "f183578e-c491-4278-f86b-a2e70c3ba65c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag of Wordsの作成\n",
        "bow_corpus = [dictionary.doc2bow(d) for d in documents]\n",
        "\n",
        "# (id, 出現回数)のリスト\n",
        "print(bow_corpus)\n",
        "\n",
        "print()\n",
        "# bow_corpusの内容をわかりやすく出力する\n",
        "texts = [text1, text2, text3]\n",
        "\n",
        "for i in range(len(bow_corpus)):\n",
        "    print(texts[i])\n",
        "    for j in range(len(bow_corpus[i])):\n",
        "        index = bow_corpus[i][j][0]\n",
        "        num = bow_corpus[i][j][1]\n",
        "        print(\"\\\"\", dictionary[index], \"\\\" が \" ,num, \"回\", end=\", \")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4qIKC5437m1",
        "outputId": "4ac41609-02ce-404d-9e27-096be5f57488"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[], [], []]\n",
            "\n",
            "すもももももももものうち\n",
            "\n",
            "料理も景色もすばらしい\n",
            "\n",
            "私の趣味は写真撮影です\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ベクトル表現② tf-idf\n",
        "特定の文書にのみ多く出現する単語⇒重要度が高くなる。"
      ],
      "metadata": {
        "id": "FnEl0tENx21P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "特定の文書にしか出現しない単語の重要度　　　⇒上げる。｢黄金｣ 「回転」 「レッスン」\n",
        "\n",
        "多くの文書に出現する語（一般的な語）の重要度⇒下げる。「です」「ます」\n",
        "\n",
        "\n",
        "ジョジョ７部は「黄金」「回転」という単語が特融で頻出する⇒<font color=\"red\">重要度が高い</font>"
      ],
      "metadata": {
        "id": "dMsQRPB3-wX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 実装\n",
        "[こちらから](https://premium.aidemy.jp/courses/5050/exercises/HydX93IoUxz)"
      ],
      "metadata": {
        "id": "vtummxEo_oNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "その①"
      ],
      "metadata": {
        "id": "paTceGlAGbCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "Kh5A8V1HvBkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 表示するときに有効数字２桁で表示する np.roundみたいに小数点を省略している。\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "\n",
        "tex1 = \"白　黒　赤\"\n",
        "tex2 = \"白　白　黒\"\n",
        "tex3 = \"赤　黒\"\n",
        "\n",
        "# 分かち書きされた文書 を格納。\n",
        "docs = np.array([\n",
        "    tex1, \n",
        "    tex2, \n",
        "    tex3\n",
        "])"
      ],
      "metadata": {
        "id": "uuGwo_l0ylNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ベクトル表現化を行う変換器を生成 引数　use_idf=True: tfとidf双方の重みづけ。　 \n",
        "# token_pattern=\"(?u)\\\\b\\\\w+\\\\b\"⇒特に考えなくて宜しだが　　(?u)は Unicode、\\bは空白、\\w は文字、+は直前の文字が1文字以上続く意味で、空白で挟まれた文字列を抽出する意味\n",
        "vectorizer = TfidfVectorizer(use_idf=True, token_pattern=\"(?u)\\\\b\\\\w+\\\\b\")\n",
        "\n",
        "# 文書をベクトルに変換。 引数には分かち書きされた文章を格納。\n",
        "vecs = vectorizer.fit_transform(docs)\n",
        "\n",
        "# 列要素を取得します\n",
        "print(vectorizer.get_feature_names())\n",
        "\n",
        "# tf-idf値を格納した行列を取得します\n",
        "print(vecs.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1YnUK3RAUot",
        "outputId": "c6afb24a-8278-4473-ae1e-7d63a404a417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['白', '赤', '黒']\n",
            "[[0.62 0.62 0.48]\n",
            " [0.93 0.   0.36]\n",
            " [0.   0.79 0.61]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "文字コード順。[こちらの回答](https://aidemy-communication.slack.com/archives/C03ANT80SAE/p1662207583380019?thread_ts=1662203811.211619&cid=C03ANT80SAE)"
      ],
      "metadata": {
        "id": "CryNk_mQTTTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 文字コード順。\n",
        "print('白：','白'.encode('utf-8'))\n",
        "\n",
        "# 白： b'\\xe7\\x99\\xbd'\n",
        "# 赤： b'\\xe8\\xb5\\xa4'\n",
        "# 黒： b'\\xe9\\xbb\\x92'\n",
        "# 青： b'\\xe9\\x9d\\x92'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piFnmUgHTbCX",
        "outputId": "9cc32c6b-43b7-4d82-83cf-7152d136896e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "白： b'\\xe7\\x99\\xbd'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "その②"
      ],
      "metadata": {
        "id": "8Gv01XgLGZch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "np.set_printoptions(precision=2) # 桁数を丸める。\n",
        "\n",
        "tex5 =  \"リンゴ リンゴ\"\n",
        "tex6 =  \"リンゴ ゴリラ\"\n",
        "tex7 =  \"ゴリラ ラッパ\"\n",
        "\n",
        "docs = np.array([tex5,\n",
        "                 tex6,\n",
        "                 tex7\n",
        "                 ])"
      ],
      "metadata": {
        "id": "OMAX6XrXGdOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ベクトル表現に変換してください。\n",
        "vectorizer = TfidfVectorizer(use_idf=True, token_pattern=\"(?u)\\\\b\\\\w+\\\\b\")\n",
        "vecs = vectorizer.fit_transform(docs)\n",
        "\n",
        "print(vectorizer.get_feature_names())\n",
        "print(vecs.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jCbT4dxBt2G",
        "outputId": "f8d60c6a-a367-46ec-8b25-97f4fad06ec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ゴリラ', 'ラッパ', 'リンゴ']\n",
            "[[0.   0.   1.  ]\n",
            " [0.71 0.   0.71]\n",
            " [0.61 0.8  0.  ]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ベクトル表現③ COS類似度\n",
        " 文字を数値化してその文字間の類似度を数値として表現する。つまり、文書のベクトル化⇒文書同士の類似度を解析。\n",
        "\n",
        " [Aidemyの教材](https://premium.aidemy.jp/courses/5050/exercises/BJK793LjIlf)"
      ],
      "metadata": {
        "id": "UttQRJ-5x29t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**「1に近い場合は似ていて、0に近いときは似ていない」**⇒これを抑えていこう。"
      ],
      "metadata": {
        "id": "cBML1jGyDg2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 実装"
      ],
      "metadata": {
        "id": "Vl-A2zD3D3wX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peFCmmJdx29z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(v1, v2):\n",
        "    cos_sim = np.dot(v1, v2) / (np.linalg.norm(v1)*np.linalg.norm(v2))\n",
        "    return cos_sim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"リンゴ リンゴ\",\n",
        "\n",
        " \"リンゴ ゴリラ\",\n",
        " \n",
        "  \"ゴリラ ラッパ\"\n",
        "  \n",
        "  という文の類似度を計算して出力してみましょう。"
      ],
      "metadata": {
        "id": "bfpCLv3dENzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "np.set_printoptions(precision=2) # 桁数を丸める。​\n",
        "tex5 =  \"リンゴ リンゴ\"\n",
        "tex6 =  \"リンゴ ゴリラ\"\n",
        "tex7 =  \"ゴリラ ラッパ\"\n",
        "\n",
        "docs = np.array([tex5, tex6, tex7])"
      ],
      "metadata": {
        "id": "Ayo1tJQ8Jl6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(use_idf=True, token_pattern=\"(?u)\\\\b\\\\w+\\\\b\")\n",
        "\n",
        "# 文書をベクトルに変換。 引数には分かち書きされた文章を格納。\n",
        "vecs = vectorizer.fit_transform(docs)\n",
        "\n",
        "# tf-idf値を格納した行列を取得します\n",
        "print(vecs.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cnq51_ENJ0Bl",
        "outputId": "b219852e-b32b-47b5-c4cf-f4cbf07ead16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.   0.   1.  ]\n",
            " [0.71 0.   0.71]\n",
            " [0.61 0.8  0.  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cos類似度を求める関数を定義してください。\n",
        "def cosine_similarity(v1, v2):\n",
        "    cos_sim = np.dot(v1, v2) / (np.linalg.norm(v1)*np.linalg.norm(v2))\n",
        "    return cos_sim"
      ],
      "metadata": {
        "id": "zYk88y2QKHc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 類似度を比較してみましょう。\n",
        "# print(\"{:1.3F}\".format(cosine_similarity(tex5, tex6)))\n",
        "# print(\"{:1.3F}\".format(cosine_similarity(tex5, tex7)))\n",
        "# print(\"{:1.3F}\".format(cosine_similarity(tex6, tex7)))\n",
        "\n",
        "# 類似度を比較してみましょう。\n",
        "print(\"{:1.3F}\".format(cosine_similarity(vecs[0], vecs[1])))\n",
        "print(\"{:1.3F}\".format(cosine_similarity(vecs[0], vecs[2])))\n",
        "print(\"{:1.3F}\".format(cosine_similarity(vecs[1], vecs[2])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj66ktudEaFX",
        "outputId": "2b9f1a26-56bb-4b6e-9c98-0d4ebbc3fb26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.707\n",
            "0.000\n",
            "0.428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tf-idf値を格納した行列を取得します\n",
        "vecs = vecs.toarray()\n",
        "# print(vecs.toarray())​\n",
        "# cos類似度を求める関数を定義してください。\n",
        "def cosine_similarity(v1, v2):\n",
        "    cos_sim = np.dot(v1, v2) / (np.linalg.norm(v1)*np.linalg.norm(v2))\n",
        "    return cos_sim\n",
        "\n",
        "# 類似度を比較してみましょう。\n",
        "print(\"{:1.3F}\".format(cosine_similarity(vecs[0], vecs[1])))\n",
        "print(\"{:1.3F}\".format(cosine_similarity(vecs[0], vecs[2])))\n",
        "print(\"{:1.3F}\".format(cosine_similarity(vecs[1], vecs[2])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2029vJLeRFv7",
        "outputId": "295ed509-1c57-4323-a0ba-bfe58afe3d8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.707\n",
            "0.000\n",
            "0.428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 単語のベクトル表現\n",
        "文章と何が違うのか分からないけど。"
      ],
      "metadata": {
        "id": "OUhQWm5Kx4Ik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "単語をベクトルで表現⇒単語の意味の近さの数値化を行う。同義語の探索等に活用。\n",
        "\n",
        "これを使用すると…単語と単語の関係性を表現できる。 <font color=\"red\">単語同士の演算</font>も可能。\n",
        "\n",
        "```\n",
        "「王様」 - 「男」+ 「女」 = 「女王」\n",
        "「パリ」 - 「フランス」 + 「日本」 = 「東京」\n",
        "```\n",
        "\n",
        "上記をニュアンスやイメージではなく数式として行う事。\n"
      ],
      "metadata": {
        "id": "r6XmpUiHl-RB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec \n",
        "単語をベクトル表現するツール。更に次元を圧縮も可能。"
      ],
      "metadata": {
        "id": "IqNTTOk9oM_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "例：<font color=\"red\">\"男\"</font>　という単語と関連性が高い文字列を調べたい。\n",
        "\n",
        "①ニュース記事をテキストコーパスとして抽出。文章とカテゴリーに分割。\n",
        ": 「globモジュール」、「with文」、「コーパスの呼び出し」\n",
        "\n",
        "②取り出した文書を品詞ごとに分割し、リストにする。: 「janome(3)」\n",
        "\n",
        "③Word2Vecでモデルを生成。: 「Word2Vec（実装）」\n",
        "\n",
        "④男との関連性が高い語を調べる。: 「Word2Vec（実装）」"
      ],
      "metadata": {
        "id": "Z7Cl6OzIoTs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 実装"
      ],
      "metadata": {
        "id": "DH_VRS5mypfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# module import\n",
        "from gensim.models import word2vec"
      ],
      "metadata": {
        "id": "WibPDq8Jy1mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルを生成：学習に使用するリスト（分かち書きされた文書）を Word2Vec関数 の引数とする\n",
        "model = word2vec.Word2Vec(リスト, size=a, min_count=b, window=c)\n",
        "# ただし、a, b, cは数字"
      ],
      "metadata": {
        "id": "X_ogmakNzVOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "・size ：ベクトルの次元数。\n",
        "\n",
        "・window ：この数の前後の単語を、関連性のある単語と見なして学習を行う。\n",
        "\n",
        "・min_count ：n回未満登場する単語を破棄。"
      ],
      "metadata": {
        "id": "nrmctbwdzXrx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "①「BOW カウント」等で扱ったjanome.tokenizerを使って予め 分かち書き を行います。 "
      ],
      "metadata": {
        "id": "J7EjJ8CQzLHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#実例\n",
        "\n",
        "import glob\n",
        "from janome.tokenizer import Tokenizer\n",
        "from gensim.models import word2vec\n",
        "\n",
        "\n",
        "def load_livedoor_news_corpus():\n",
        "    category = {\n",
        "        \"peachy\":1,\n",
        "        \"smax\":2\n",
        "    }\n",
        "    texts  = []\n",
        "    labels = []\n",
        "    \n",
        "    # 以下に説明文のコードの該当箇所を写してください\n",
        "    #-----------------------------------\n",
        "    for name, label in category.items():\n",
        "        files = glob.glob(\"./5050_nlp_data/{name}/{name}*.txt\".format(name=name))\n",
        "        for file in files:\n",
        "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
        "                lines = f.read().splitlines() \n",
        "                text = \"\".join(lines[2:])\n",
        "            texts.append(text)\n",
        "            labels.append(label)\n",
        "\n",
        "    return texts, labels\n",
        "    #-----------------------------------\n",
        "\n",
        "#texts, labels = load_livedoor_news_corpus()\n",
        "\n",
        "# 品詞を取り出し「名詞、動詞、形容詞、形容動詞」のリスト作成\n",
        "def tokenize(part):\n",
        "    tokens = t.tokenize(\",\".join(part))\n",
        "    word = []\n",
        "    for token in tokens:\n",
        "        part_of_speech = token.part_of_speech.split(\",\")[0]\n",
        " \n",
        "        if part_of_speech in [\"名詞\", \"動詞\", \"形容詞\", \"形容動詞\"]:\n",
        "            word.append(token.surface)            \n",
        "    return word\n",
        "\n",
        "# ラベルと文章に分類\n",
        "texts, labels = load_livedoor_news_corpus()\n",
        "t = Tokenizer() # 最初にTokenizerインスタンスを作成する\n",
        "sentences = tokenize(texts[0:100])  # データ量が多いため制限している\n",
        "# 以下に回答を作成してください\n",
        "#word2vec.Word2Vecの引数に関して、size=100, min_count=5, window=15としてください\n",
        "model = word2vec.Word2Vec([sentences], size=100, min_count=5, window=15)\n",
        "print(model.most_similar(positive=[\"男\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "meyqmyEqziq7",
        "outputId": "b227b4c9-b2e8-43f1-a0cc-79dc4e7ddd0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-109-7b74d8de35a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# 以下に回答を作成してください\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m#word2vec.Word2Vecの引数に関して、size=100, min_count=5, window=15としてください\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"男\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m             total_words=total_words, **kwargs)\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[0;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# should be set by `build_vocab`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must first build vocabulary before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must initialize vectors before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#globモジュール"
      ],
      "metadata": {
        "id": "armW7nk8oNR9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ファイルやディレクトリを操作するときに便利なモジュール。"
      ],
      "metadata": {
        "id": "LZelHLVSp0W3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "名前の確認。\n",
        "\n",
        "・**ファイル**：文書、写真、音楽など、ユーザーが操作・管理する情報の最小単位。\n",
        "\n",
        "・**ディレクトリ**：ファイルをまとめる入れ物のこと。\n",
        "\n",
        "・**パス**：コンピュータ上でファイルやディレクトリの場所のこと。"
      ],
      "metadata": {
        "id": "_lx85rnhr18Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "特殊な文字や文字列を用いて賢くファイルを検索できる点。どういう事？？"
      ],
      "metadata": {
        "id": "bjoayVq_slh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "特定のファイルを簡単に取得することができる。\n",
        "https://www.youtube.com/watch?v=86C5NDP1dZs"
      ],
      "metadata": {
        "id": "VRcKsDjptD57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "lis = glob.glob(\"test/sample[0-9].txt\")\n",
        "print(lis)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1upQOe4rpi3",
        "outputId": "9bb28faa-287f-421f-b847-4212122c75c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['test/sample1.txt', 'test/sample2.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "例。\n",
        "\n",
        "①「5050_nlp_dataディレクトリ」の中にある、テキストファイルを出力する方法。\n",
        "\n",
        "②更にリストのはじめの3つの要素を出力するには…"
      ],
      "metadata": {
        "id": "YV7MVfz9tZbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "# 5050_nlp_data/sports-watchの中にあるファイルを表示してください\n",
        "lis = glob.glob(\"5050_nlp_data/sports-watch/*.txt\")#ディレクトリ内のファイルを表示。\n",
        "\n",
        "print(lis[0:3])"
      ],
      "metadata": {
        "id": "aZTdvYRNuXC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# with分。\n",
        "テキスト開きでも少しやりました。⇒使いこなせるように。"
      ],
      "metadata": {
        "id": "FSU67DcDoNkk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "通常のファイル開き。\n",
        "\n",
        "**open()**でファイルを開き、　**read()**等でファイルを読み込み、　**close()**でファイルを閉じます。"
      ],
      "metadata": {
        "id": "jAjTbIxYussj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "それよりも楽な with文。\n",
        "\n",
        "ファイルが自動的にclose()されたり、ファイルを開いている際にエラーが発生しても適切な例外処理が自動的に行われるので、とても便利"
      ],
      "metadata": {
        "id": "-0jy_TFiu3hL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"a.text\", \"r\", encoding=\"utf-8\") as f:\n",
        "  data = f.read()"
      ],
      "metadata": {
        "id": "22qU7pvJvmI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with文を用いて5050_nlp_data/sports-watch上にあるLICENSE.txtを開き、出力してください\n",
        "with open(\"5050_nlp_data/sports-watch/LICENSE.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    print(f.read())"
      ],
      "metadata": {
        "id": "QpGVwT64vr1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# コーパスの取り出し\n",
        "多分あまり重要じゃない。"
      ],
      "metadata": {
        "id": "lpEoFNHAwqmE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**コーパス**： 文書または音声データにある種の情報を与えたデータ の事。"
      ],
      "metadata": {
        "id": "AndISwyGwtx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "そのようなコーパスを取り出して、分類する事ができます。"
      ],
      "metadata": {
        "id": "oRsKPSjgw-E-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lYyPSMJCxSxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "def load_livedoor_news_corpus():\n",
        "    category = {\n",
        "        \"dokujo-tsushin\": 1,\n",
        "        \"it-life-hack\":2,\n",
        "        \"kaden-channel\": 3,\n",
        "        \"livedoor-homme\": 4,\n",
        "        \"movie-enter\": 5,\n",
        "        \"peachy\": 6,\n",
        "        \"smax\": 7,\n",
        "        \"sports-watch\": 8,\n",
        "        \"topic-news\":9\n",
        "    }\n",
        "    texts  = [] # 全ての記事の文章をここに格納します。\n",
        "    labels = [] # docsに格納される記事の1〜9のカテゴリーを、ラベルとして扱います。\n",
        "\n",
        "    # 全てのカテゴリーのディレクトリについて実行します。\n",
        "    for name, label in category.items():\n",
        "        # {c_name}にcategory.items()から取得したカテゴリー名c_nameをformatメソッドを用いて埋め込みます。\n",
        "        files = glob.glob(\"./5050_nlp_data/{name}/{name}*.txt\".format(name=name))\n",
        "        # 各記事について、本文の情報を以下のように取得します。\n",
        "        for file in files:\n",
        "            # with文を用いるため、close()は不要です。\n",
        "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
        "                # 改行文字で分割\n",
        "                lines = f.read().splitlines()\n",
        "                # 分割すると0番目にurl, 1番目に日付、2番目にタイトル、3番目以降に記事本文が記載されています。\n",
        "                # 記事中の本文を1行にまとめてしまいます。\n",
        "                text = \"\".join(lines[2:])\n",
        "\n",
        "            texts.append(text)\n",
        "            labels.append(label)\n",
        "\n",
        "    return texts, labels\n",
        "    \n",
        "# 全ての記事の文章データとそのラベル（カテゴリー）を取得します。\n",
        "texts, labels = load_livedoor_news_corpus()"
      ],
      "metadata": {
        "id": "XWtVGsNFxSDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Doc2Vec\n",
        "Word2Vecを応用した 文章をベクトル化する技術"
      ],
      "metadata": {
        "id": "jSyC074kwq1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BOWとの大きな違いは 文の語順 も特徴として考慮に入れることができる⇒BOWとの違い。"
      ],
      "metadata": {
        "id": "oNy0BPWq0pkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "実装。"
      ],
      "metadata": {
        "id": "YbnIdsmJ1HUm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "①分かち書き\n",
        "\n",
        "文章を**janomeのTokenizer**を用い、分かち書きにします。"
      ],
      "metadata": {
        "id": "kFNnbk7t1JGm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "②TaggedDocument クラスのインスタンスを作成\n",
        "\n",
        "TaggedDocumentの引数にwords=\"分かち書きされた各要素\", tags=[\"タグ\"]を与える.\n",
        "\n",
        "⇒TaggedDocument クラスのインスタンスを作成。　\n",
        "これをTaggedDocumentをリストに格納し、Doc2Vecに渡す。"
      ],
      "metadata": {
        "id": "CodgoTdY1TO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "③Doc2Vec でモデルの生成\n"
      ],
      "metadata": {
        "id": "P_jMgudh1ud9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#モデルの学習。\n",
        "model = Doc2Vec(documents=リスト, min_count=1)\n",
        "\n",
        "# min_count:最低この回数出現した単語のみを学習に使用"
      ],
      "metadata": {
        "id": "2orxuiOj1xRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "④類似度の出力"
      ],
      "metadata": {
        "id": "-t3waOZ2146G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(4):\n",
        "  print(model.docvecs.most_similar(\"d\"+str(i)))"
      ],
      "metadata": {
        "id": "slASgn2j17N7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "実例："
      ],
      "metadata": {
        "id": "zP843GXq2Ey2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from gensim.models.doc2vec import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from janome.tokenizer import Tokenizer\n",
        "\n",
        "def load_livedoor_news_corpus():\n",
        "    category = {\n",
        "        \"dokujo-tsushin\": 1,\n",
        "        \"sports-watch\": 2,\n",
        "    }\n",
        "    texts  = []\n",
        "    labels = []\n",
        "    \n",
        "    # 以下に説明文のコードの該当箇所を写してください\n",
        "    #-----------------------------------\n",
        "    for name, label in category.items():\n",
        "        files = glob.glob(\"./5050_nlp_data/{name}/{name}*.txt\".format(name=name))\n",
        "        for file in files:\n",
        "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
        "                lines = f.read().splitlines() \n",
        "                text = \"\".join(lines[2:])\n",
        "            texts.append(text)\n",
        "            labels.append(label)\n",
        "\n",
        "    return texts, labels\n",
        "    #-----------------------------------\n",
        "\n",
        "texts, labels = load_livedoor_news_corpus()\n",
        "# Doc2Vecの処理\n",
        "token = [] # 各docsの分かち書きした結果を格納するリストです\n",
        "training_docs = [] # TaggedDocumentを格納するリストです\n",
        "t = Tokenizer() # 最初にTokenizerインスタンスを作成する\n",
        "for i in range(4):\n",
        "    \n",
        "    # docs[i] を分かち書きして、tokenに格納します\n",
        "    token.append(t.tokenize(texts[i], wakati=True))\n",
        "    \n",
        "    # TaggedDocument クラスのインスタンスを作成して、結果をtraining_docsに格納します\n",
        "    # タグは \"d番号\"とします\n",
        "    training_docs.append(TaggedDocument(words=token[i], tags=[\"d\" + str(i)]))\n",
        "\n",
        "# 以下に回答を作成してください\n",
        "model = Doc2Vec(documents=training_docs, min_count=1)\n",
        "\n",
        "for i in range(4):\n",
        "    print(model.docvecs.most_similar(\"d\"+str(i)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "mbwXaUEn2Gcp",
        "outputId": "15932f52-d22b-472f-a8a6-e90be8536c7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e7c939dbcc6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTaggedDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjanome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_livedoor_news_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'janome'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}